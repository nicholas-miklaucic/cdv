{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import rho_plus as rp\n",
    "\n",
    "is_dark = False\n",
    "theme, cs = rp.mpl_setup(is_dark)\n",
    "rp.plotly_setup(is_dark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/nmiklaucic/cdv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nmiklaucic/miniconda3/envs/avid/lib/python3.12/site-packages/IPython/core/magics/osm.py:393: UserWarning: This is now an optional IPython functionality, using bookmarks requires you to install the `pickleshare` library.\n",
      "  bkms = self.shell.db.get('bookmarks', {})\n",
      "/home/nmiklaucic/miniconda3/envs/avid/lib/python3.12/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "cd ~/cdv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('data/ICSD_CN/train.txt')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "root = Path('data/ICSD_CN/')\n",
    "\n",
    "train = root / 'train.txt'\n",
    "valid = root / 'validation.txt'\n",
    "test = root / 'test.txt'\n",
    "\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'atoms', 'labels'],\n",
       "        num_rows: 22220\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'atoms', 'labels'],\n",
       "        num_rows: 2621\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['text', 'atoms', 'labels'],\n",
       "        num_rows: 1303\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('text', data_files={'train': str(train), 'test': str(test), 'valid': str(valid)}, sample_by='paragraph')\n",
    "def split(sample):\n",
    "    lines = sample['text'].split('\\n')\n",
    "    atoms = [line.split(' ')[0] for line in lines]\n",
    "    classes = [line.split(' ')[1] for line in lines]\n",
    "    return {\n",
    "        'atoms': atoms,\n",
    "        'labels': classes\n",
    "    }\n",
    "dataset = dataset.map(split)\n",
    "dataset = dataset.filter(lambda ex: len(ex['atoms']) <= 64)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['K 1\\nK 1\\nCd 2\\nCd 2\\nCd 2\\nCd 2\\nCd 2\\nCd 2\\nH 1\\nH 1\\nH 1\\nH 1\\nH 1\\nH 1\\nH 1\\nH 1\\nH 1\\nH 1\\nH 1\\nH 1\\nH 1\\nH 1\\nH 1\\nH 1\\nCl -1\\nCl -1\\nCl -1\\nCl -1\\nCl -1\\nCl -1\\nCl -1\\nCl -1\\nCl -1\\nCl -1\\nCl -1\\nCl -1\\nCl -1\\nCl -1\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2',\n",
       "  'Ca 2\\nCa 2\\nCa 2\\nCa 2\\nNi 2\\nNi 2\\nNi 2\\nNi 2\\nAs 5\\nAs 5\\nAs 5\\nAs 5\\nH 1\\nH 1\\nH 1\\nH 1\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2',\n",
       "  'Ce 3\\nCe 3\\nCe 3\\nCe 3\\nCe 3\\nCe 3\\nCe 3\\nCe 3\\nDy 3\\nDy 3\\nDy 3\\nDy 3\\nDy 3\\nDy 3\\nDy 3\\nDy 3\\nS -2\\nS -2\\nS -2\\nS -2\\nS -2\\nS -2\\nS -2\\nS -2\\nS -2\\nS -2\\nS -2\\nS -2\\nS -2\\nS -2\\nS -2\\nS -2\\nS -2\\nS -2\\nS -2\\nS -2\\nS -2\\nS -2\\nS -2\\nS -2',\n",
       "  'Er 3\\nEr 3\\nH 1\\nH 1\\nH 1\\nH 1\\nH 1\\nH 1\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2',\n",
       "  'H 1\\nH 1\\nH 1\\nH 1\\nH 1\\nH 1\\nH 1\\nH 1\\nH 1\\nH 1\\nH 1\\nH 1\\nH 1\\nH 1\\nH 1\\nH 1\\nH 1\\nH 1\\nH 1\\nH 1\\nCl 7\\nCl 7\\nCl 7\\nCl 7\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2',\n",
       "  'Ag 2\\nAg 2\\nB 3\\nB 3\\nF -1\\nF -1\\nF -1\\nF -1\\nF -1\\nF -1\\nF -1\\nF -1\\nF -1\\nF -1',\n",
       "  'Gd 3\\nBi 3\\nBi 3\\nCl -1\\nO -2\\nO -2\\nO -2\\nO -2',\n",
       "  'Mg 2\\nMg 2\\nMg 2\\nAl 3\\nAl 3\\nAl 3\\nAl 3\\nAl 3\\nAl 3\\nS -2\\nS -2\\nS -2\\nS -2\\nS -2\\nS -2\\nS -2\\nS -2\\nS -2\\nS -2\\nS -2\\nS -2',\n",
       "  'Rb 1\\nRb 1\\nRb 1\\nRb 1\\nRb 1\\nRb 1\\nRb 1\\nRb 1\\nMn 2\\nMn 2\\nMn 2\\nMn 2\\nMn 2\\nMn 2\\nMn 2\\nMn 2\\nV 5\\nV 5\\nV 5\\nV 5\\nV 5\\nV 5\\nV 5\\nV 5\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2',\n",
       "  'H 1\\nH 1\\nH 1\\nH 1\\nH 1\\nH 1\\nH 1\\nH 1\\nH 1\\nH 1\\nH 1\\nH 1\\nH 1\\nH 1\\nH 1\\nH 1\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2',\n",
       "  'Eu 3\\nEu 3\\nEu 3\\nEu 3\\nTi 4\\nTi 4\\nTi 4\\nTi 4\\nCl -1\\nCl -1\\nCl -1\\nCl -1\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2',\n",
       "  'Rb 1\\nRb 1\\nRb 1\\nRb 1\\nPb 2\\nPb 2\\nPb 2\\nPb 2\\nF -1\\nF -1\\nF -1\\nF -1\\nF -1\\nF -1\\nF -1\\nF -1\\nF -1\\nF -1\\nF -1\\nF -1',\n",
       "  'Tl 1\\nTl 1\\nBi 3\\nBi 3\\nBi 3\\nBi 3\\nCl -1\\nCl -1\\nCl -1\\nCl -1\\nCl -1\\nCl -1\\nCl -1\\nCl -1\\nCl -1\\nCl -1\\nCl -1\\nCl -1\\nCl -1\\nCl -1',\n",
       "  'U 6\\nU 6\\nTe -2\\nTe -2\\nTe -2\\nTe -2\\nTe -2\\nTe -2',\n",
       "  'Nb 4\\nNb 4\\nSe -2\\nSe -2\\nSe -2\\nSe -2\\nSe -2\\nSe -2\\nSe -2\\nSe -2\\nNb 4\\nNb 4',\n",
       "  'Ca 2\\nCa 2\\nCa 2\\nCa 2\\nCa 2\\nCa 2\\nCa 2\\nCa 2\\nCa 2\\nCa 2\\nCa 2\\nCa 2\\nRu 4\\nRu 4\\nRu 4\\nRu 4\\nRu 4\\nRu 4\\nRu 4\\nRu 4\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2'],\n",
       " 'atoms': [['K',\n",
       "   'K',\n",
       "   'Cd',\n",
       "   'Cd',\n",
       "   'Cd',\n",
       "   'Cd',\n",
       "   'Cd',\n",
       "   'Cd',\n",
       "   'H',\n",
       "   'H',\n",
       "   'H',\n",
       "   'H',\n",
       "   'H',\n",
       "   'H',\n",
       "   'H',\n",
       "   'H',\n",
       "   'H',\n",
       "   'H',\n",
       "   'H',\n",
       "   'H',\n",
       "   'H',\n",
       "   'H',\n",
       "   'H',\n",
       "   'H',\n",
       "   'Cl',\n",
       "   'Cl',\n",
       "   'Cl',\n",
       "   'Cl',\n",
       "   'Cl',\n",
       "   'Cl',\n",
       "   'Cl',\n",
       "   'Cl',\n",
       "   'Cl',\n",
       "   'Cl',\n",
       "   'Cl',\n",
       "   'Cl',\n",
       "   'Cl',\n",
       "   'Cl',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O'],\n",
       "  ['Ca',\n",
       "   'Ca',\n",
       "   'Ca',\n",
       "   'Ca',\n",
       "   'Ni',\n",
       "   'Ni',\n",
       "   'Ni',\n",
       "   'Ni',\n",
       "   'As',\n",
       "   'As',\n",
       "   'As',\n",
       "   'As',\n",
       "   'H',\n",
       "   'H',\n",
       "   'H',\n",
       "   'H',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O'],\n",
       "  ['Ce',\n",
       "   'Ce',\n",
       "   'Ce',\n",
       "   'Ce',\n",
       "   'Ce',\n",
       "   'Ce',\n",
       "   'Ce',\n",
       "   'Ce',\n",
       "   'Dy',\n",
       "   'Dy',\n",
       "   'Dy',\n",
       "   'Dy',\n",
       "   'Dy',\n",
       "   'Dy',\n",
       "   'Dy',\n",
       "   'Dy',\n",
       "   'S',\n",
       "   'S',\n",
       "   'S',\n",
       "   'S',\n",
       "   'S',\n",
       "   'S',\n",
       "   'S',\n",
       "   'S',\n",
       "   'S',\n",
       "   'S',\n",
       "   'S',\n",
       "   'S',\n",
       "   'S',\n",
       "   'S',\n",
       "   'S',\n",
       "   'S',\n",
       "   'S',\n",
       "   'S',\n",
       "   'S',\n",
       "   'S',\n",
       "   'S',\n",
       "   'S',\n",
       "   'S',\n",
       "   'S'],\n",
       "  ['Er', 'Er', 'H', 'H', 'H', 'H', 'H', 'H', 'O', 'O', 'O', 'O', 'O', 'O'],\n",
       "  ['H',\n",
       "   'H',\n",
       "   'H',\n",
       "   'H',\n",
       "   'H',\n",
       "   'H',\n",
       "   'H',\n",
       "   'H',\n",
       "   'H',\n",
       "   'H',\n",
       "   'H',\n",
       "   'H',\n",
       "   'H',\n",
       "   'H',\n",
       "   'H',\n",
       "   'H',\n",
       "   'H',\n",
       "   'H',\n",
       "   'H',\n",
       "   'H',\n",
       "   'Cl',\n",
       "   'Cl',\n",
       "   'Cl',\n",
       "   'Cl',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O'],\n",
       "  ['Ag', 'Ag', 'B', 'B', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F'],\n",
       "  ['Gd', 'Bi', 'Bi', 'Cl', 'O', 'O', 'O', 'O'],\n",
       "  ['Mg',\n",
       "   'Mg',\n",
       "   'Mg',\n",
       "   'Al',\n",
       "   'Al',\n",
       "   'Al',\n",
       "   'Al',\n",
       "   'Al',\n",
       "   'Al',\n",
       "   'S',\n",
       "   'S',\n",
       "   'S',\n",
       "   'S',\n",
       "   'S',\n",
       "   'S',\n",
       "   'S',\n",
       "   'S',\n",
       "   'S',\n",
       "   'S',\n",
       "   'S',\n",
       "   'S'],\n",
       "  ['Rb',\n",
       "   'Rb',\n",
       "   'Rb',\n",
       "   'Rb',\n",
       "   'Rb',\n",
       "   'Rb',\n",
       "   'Rb',\n",
       "   'Rb',\n",
       "   'Mn',\n",
       "   'Mn',\n",
       "   'Mn',\n",
       "   'Mn',\n",
       "   'Mn',\n",
       "   'Mn',\n",
       "   'Mn',\n",
       "   'Mn',\n",
       "   'V',\n",
       "   'V',\n",
       "   'V',\n",
       "   'V',\n",
       "   'V',\n",
       "   'V',\n",
       "   'V',\n",
       "   'V',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O'],\n",
       "  ['H',\n",
       "   'H',\n",
       "   'H',\n",
       "   'H',\n",
       "   'H',\n",
       "   'H',\n",
       "   'H',\n",
       "   'H',\n",
       "   'H',\n",
       "   'H',\n",
       "   'H',\n",
       "   'H',\n",
       "   'H',\n",
       "   'H',\n",
       "   'H',\n",
       "   'H',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O'],\n",
       "  ['Eu',\n",
       "   'Eu',\n",
       "   'Eu',\n",
       "   'Eu',\n",
       "   'Ti',\n",
       "   'Ti',\n",
       "   'Ti',\n",
       "   'Ti',\n",
       "   'Cl',\n",
       "   'Cl',\n",
       "   'Cl',\n",
       "   'Cl',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O'],\n",
       "  ['Rb',\n",
       "   'Rb',\n",
       "   'Rb',\n",
       "   'Rb',\n",
       "   'Pb',\n",
       "   'Pb',\n",
       "   'Pb',\n",
       "   'Pb',\n",
       "   'F',\n",
       "   'F',\n",
       "   'F',\n",
       "   'F',\n",
       "   'F',\n",
       "   'F',\n",
       "   'F',\n",
       "   'F',\n",
       "   'F',\n",
       "   'F',\n",
       "   'F',\n",
       "   'F'],\n",
       "  ['Tl',\n",
       "   'Tl',\n",
       "   'Bi',\n",
       "   'Bi',\n",
       "   'Bi',\n",
       "   'Bi',\n",
       "   'Cl',\n",
       "   'Cl',\n",
       "   'Cl',\n",
       "   'Cl',\n",
       "   'Cl',\n",
       "   'Cl',\n",
       "   'Cl',\n",
       "   'Cl',\n",
       "   'Cl',\n",
       "   'Cl',\n",
       "   'Cl',\n",
       "   'Cl',\n",
       "   'Cl',\n",
       "   'Cl'],\n",
       "  ['U', 'U', 'Te', 'Te', 'Te', 'Te', 'Te', 'Te'],\n",
       "  ['Nb', 'Nb', 'Se', 'Se', 'Se', 'Se', 'Se', 'Se', 'Se', 'Se', 'Nb', 'Nb'],\n",
       "  ['Ca',\n",
       "   'Ca',\n",
       "   'Ca',\n",
       "   'Ca',\n",
       "   'Ca',\n",
       "   'Ca',\n",
       "   'Ca',\n",
       "   'Ca',\n",
       "   'Ca',\n",
       "   'Ca',\n",
       "   'Ca',\n",
       "   'Ca',\n",
       "   'Ru',\n",
       "   'Ru',\n",
       "   'Ru',\n",
       "   'Ru',\n",
       "   'Ru',\n",
       "   'Ru',\n",
       "   'Ru',\n",
       "   'Ru',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O']],\n",
       " 'labels': [['1',\n",
       "   '1',\n",
       "   '2',\n",
       "   '2',\n",
       "   '2',\n",
       "   '2',\n",
       "   '2',\n",
       "   '2',\n",
       "   '1',\n",
       "   '1',\n",
       "   '1',\n",
       "   '1',\n",
       "   '1',\n",
       "   '1',\n",
       "   '1',\n",
       "   '1',\n",
       "   '1',\n",
       "   '1',\n",
       "   '1',\n",
       "   '1',\n",
       "   '1',\n",
       "   '1',\n",
       "   '1',\n",
       "   '1',\n",
       "   '-1',\n",
       "   '-1',\n",
       "   '-1',\n",
       "   '-1',\n",
       "   '-1',\n",
       "   '-1',\n",
       "   '-1',\n",
       "   '-1',\n",
       "   '-1',\n",
       "   '-1',\n",
       "   '-1',\n",
       "   '-1',\n",
       "   '-1',\n",
       "   '-1',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2'],\n",
       "  ['2',\n",
       "   '2',\n",
       "   '2',\n",
       "   '2',\n",
       "   '2',\n",
       "   '2',\n",
       "   '2',\n",
       "   '2',\n",
       "   '5',\n",
       "   '5',\n",
       "   '5',\n",
       "   '5',\n",
       "   '1',\n",
       "   '1',\n",
       "   '1',\n",
       "   '1',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2'],\n",
       "  ['3',\n",
       "   '3',\n",
       "   '3',\n",
       "   '3',\n",
       "   '3',\n",
       "   '3',\n",
       "   '3',\n",
       "   '3',\n",
       "   '3',\n",
       "   '3',\n",
       "   '3',\n",
       "   '3',\n",
       "   '3',\n",
       "   '3',\n",
       "   '3',\n",
       "   '3',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2'],\n",
       "  ['3', '3', '1', '1', '1', '1', '1', '1', '-2', '-2', '-2', '-2', '-2', '-2'],\n",
       "  ['1',\n",
       "   '1',\n",
       "   '1',\n",
       "   '1',\n",
       "   '1',\n",
       "   '1',\n",
       "   '1',\n",
       "   '1',\n",
       "   '1',\n",
       "   '1',\n",
       "   '1',\n",
       "   '1',\n",
       "   '1',\n",
       "   '1',\n",
       "   '1',\n",
       "   '1',\n",
       "   '1',\n",
       "   '1',\n",
       "   '1',\n",
       "   '1',\n",
       "   '7',\n",
       "   '7',\n",
       "   '7',\n",
       "   '7',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2'],\n",
       "  ['2',\n",
       "   '2',\n",
       "   '3',\n",
       "   '3',\n",
       "   '-1',\n",
       "   '-1',\n",
       "   '-1',\n",
       "   '-1',\n",
       "   '-1',\n",
       "   '-1',\n",
       "   '-1',\n",
       "   '-1',\n",
       "   '-1',\n",
       "   '-1'],\n",
       "  ['3', '3', '3', '-1', '-2', '-2', '-2', '-2'],\n",
       "  ['2',\n",
       "   '2',\n",
       "   '2',\n",
       "   '3',\n",
       "   '3',\n",
       "   '3',\n",
       "   '3',\n",
       "   '3',\n",
       "   '3',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2'],\n",
       "  ['1',\n",
       "   '1',\n",
       "   '1',\n",
       "   '1',\n",
       "   '1',\n",
       "   '1',\n",
       "   '1',\n",
       "   '1',\n",
       "   '2',\n",
       "   '2',\n",
       "   '2',\n",
       "   '2',\n",
       "   '2',\n",
       "   '2',\n",
       "   '2',\n",
       "   '2',\n",
       "   '5',\n",
       "   '5',\n",
       "   '5',\n",
       "   '5',\n",
       "   '5',\n",
       "   '5',\n",
       "   '5',\n",
       "   '5',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2'],\n",
       "  ['1',\n",
       "   '1',\n",
       "   '1',\n",
       "   '1',\n",
       "   '1',\n",
       "   '1',\n",
       "   '1',\n",
       "   '1',\n",
       "   '1',\n",
       "   '1',\n",
       "   '1',\n",
       "   '1',\n",
       "   '1',\n",
       "   '1',\n",
       "   '1',\n",
       "   '1',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2'],\n",
       "  ['3',\n",
       "   '3',\n",
       "   '3',\n",
       "   '3',\n",
       "   '4',\n",
       "   '4',\n",
       "   '4',\n",
       "   '4',\n",
       "   '-1',\n",
       "   '-1',\n",
       "   '-1',\n",
       "   '-1',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2'],\n",
       "  ['1',\n",
       "   '1',\n",
       "   '1',\n",
       "   '1',\n",
       "   '2',\n",
       "   '2',\n",
       "   '2',\n",
       "   '2',\n",
       "   '-1',\n",
       "   '-1',\n",
       "   '-1',\n",
       "   '-1',\n",
       "   '-1',\n",
       "   '-1',\n",
       "   '-1',\n",
       "   '-1',\n",
       "   '-1',\n",
       "   '-1',\n",
       "   '-1',\n",
       "   '-1'],\n",
       "  ['1',\n",
       "   '1',\n",
       "   '3',\n",
       "   '3',\n",
       "   '3',\n",
       "   '3',\n",
       "   '-1',\n",
       "   '-1',\n",
       "   '-1',\n",
       "   '-1',\n",
       "   '-1',\n",
       "   '-1',\n",
       "   '-1',\n",
       "   '-1',\n",
       "   '-1',\n",
       "   '-1',\n",
       "   '-1',\n",
       "   '-1',\n",
       "   '-1',\n",
       "   '-1'],\n",
       "  ['6', '6', '-2', '-2', '-2', '-2', '-2', '-2'],\n",
       "  ['4', '4', '-2', '-2', '-2', '-2', '-2', '-2', '-2', '-2', '4', '4'],\n",
       "  ['2',\n",
       "   '2',\n",
       "   '2',\n",
       "   '2',\n",
       "   '2',\n",
       "   '2',\n",
       "   '2',\n",
       "   '2',\n",
       "   '2',\n",
       "   '2',\n",
       "   '2',\n",
       "   '2',\n",
       "   '4',\n",
       "   '4',\n",
       "   '4',\n",
       "   '4',\n",
       "   '4',\n",
       "   '4',\n",
       "   '4',\n",
       "   '4',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2']]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(dataset['train'].iter(batch_size=16))\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['K 1\\nK 1\\nCd 2\\nCd 2\\nCd 2\\nCd 2\\nCd 2\\nCd 2\\nH 1\\nH 1\\nH 1\\nH 1\\nH 1\\nH 1\\nH 1\\nH 1\\nH 1\\nH 1\\nH 1\\nH 1\\nH 1\\nH 1\\nH 1\\nH 1\\nCl -1\\nCl -1\\nCl -1\\nCl -1\\nCl -1\\nCl -1\\nCl -1\\nCl -1\\nCl -1\\nCl -1\\nCl -1\\nCl -1\\nCl -1\\nCl -1\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2\\nO -2'],\n",
       " 'atoms': [['K',\n",
       "   'K',\n",
       "   'Cd',\n",
       "   'Cd',\n",
       "   'Cd',\n",
       "   'Cd',\n",
       "   'Cd',\n",
       "   'Cd',\n",
       "   'H',\n",
       "   'H',\n",
       "   'H',\n",
       "   'H',\n",
       "   'H',\n",
       "   'H',\n",
       "   'H',\n",
       "   'H',\n",
       "   'H',\n",
       "   'H',\n",
       "   'H',\n",
       "   'H',\n",
       "   'H',\n",
       "   'H',\n",
       "   'H',\n",
       "   'H',\n",
       "   'Cl',\n",
       "   'Cl',\n",
       "   'Cl',\n",
       "   'Cl',\n",
       "   'Cl',\n",
       "   'Cl',\n",
       "   'Cl',\n",
       "   'Cl',\n",
       "   'Cl',\n",
       "   'Cl',\n",
       "   'Cl',\n",
       "   'Cl',\n",
       "   'Cl',\n",
       "   'Cl',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O']],\n",
       " 'labels': [['1',\n",
       "   '1',\n",
       "   '2',\n",
       "   '2',\n",
       "   '2',\n",
       "   '2',\n",
       "   '2',\n",
       "   '2',\n",
       "   '1',\n",
       "   '1',\n",
       "   '1',\n",
       "   '1',\n",
       "   '1',\n",
       "   '1',\n",
       "   '1',\n",
       "   '1',\n",
       "   '1',\n",
       "   '1',\n",
       "   '1',\n",
       "   '1',\n",
       "   '1',\n",
       "   '1',\n",
       "   '1',\n",
       "   '1',\n",
       "   '-1',\n",
       "   '-1',\n",
       "   '-1',\n",
       "   '-1',\n",
       "   '-1',\n",
       "   '-1',\n",
       "   '-1',\n",
       "   '-1',\n",
       "   '-1',\n",
       "   '-1',\n",
       "   '-1',\n",
       "   '-1',\n",
       "   '-1',\n",
       "   '-1',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2',\n",
       "   '-2']]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(dataset['train'].iter(batch_size=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_text():\n",
    "    for split, ds in dataset.items():\n",
    "        for data in ds.iter(batch_size=1):\n",
    "            yield data['atoms'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tokenizers\n",
    "from tokenizers.pre_tokenizers import WhitespaceSplit\n",
    "\n",
    "tok = tokenizers.Tokenizer(tokenizers.models.WordLevel(unk_token='[UNK]'))\n",
    "tok.train_from_iterator(all_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok.save('precomputed/tokenizer.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[9,\n",
       "  9,\n",
       "  46,\n",
       "  46,\n",
       "  46,\n",
       "  46,\n",
       "  46,\n",
       "  46,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [26,\n",
       "  26,\n",
       "  26,\n",
       "  26,\n",
       "  45,\n",
       "  45,\n",
       "  45,\n",
       "  45,\n",
       "  20,\n",
       "  20,\n",
       "  20,\n",
       "  20,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [54,\n",
       "  54,\n",
       "  54,\n",
       "  54,\n",
       "  54,\n",
       "  54,\n",
       "  54,\n",
       "  54,\n",
       "  65,\n",
       "  65,\n",
       "  65,\n",
       "  65,\n",
       "  65,\n",
       "  65,\n",
       "  65,\n",
       "  65,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2],\n",
       " [60, 60, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       " [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [23, 23, 25, 25, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       " [53, 34, 34, 4, 0, 0, 0, 0],\n",
       " [41, 41, 41, 24, 24, 24, 24, 24, 24, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
       " [22,\n",
       "  22,\n",
       "  22,\n",
       "  22,\n",
       "  22,\n",
       "  22,\n",
       "  22,\n",
       "  22,\n",
       "  27,\n",
       "  27,\n",
       "  27,\n",
       "  27,\n",
       "  27,\n",
       "  27,\n",
       "  27,\n",
       "  27,\n",
       "  29,\n",
       "  29,\n",
       "  29,\n",
       "  29,\n",
       "  29,\n",
       "  29,\n",
       "  29,\n",
       "  29,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [51,\n",
       "  51,\n",
       "  51,\n",
       "  51,\n",
       "  35,\n",
       "  35,\n",
       "  35,\n",
       "  35,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [22, 22, 22, 22, 37, 37, 37, 37, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       " [38, 38, 34, 34, 34, 34, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4],\n",
       " [48, 48, 14, 14, 14, 14, 14, 14],\n",
       " [44, 44, 6, 6, 6, 6, 6, 6, 6, 6, 44, 44],\n",
       " [26,\n",
       "  26,\n",
       "  26,\n",
       "  26,\n",
       "  26,\n",
       "  26,\n",
       "  26,\n",
       "  26,\n",
       "  26,\n",
       "  26,\n",
       "  26,\n",
       "  26,\n",
       "  67,\n",
       "  67,\n",
       "  67,\n",
       "  67,\n",
       "  67,\n",
       "  67,\n",
       "  67,\n",
       "  67,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x.ids for x in tok.encode_batch(batch['atoms'], is_pretokenized=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = dataset.map(lambda examples: {'input_ids': [x.ids for x in tok.encode_batch(examples['atoms'], is_pretokenized=True)]}, batched=True, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2      79\n",
       "3      83\n",
       "4     239\n",
       "5     318\n",
       "6     565\n",
       "     ... \n",
       "60    612\n",
       "61      9\n",
       "62    159\n",
       "63     32\n",
       "64    689\n",
       "Name: count, Length: 63, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(list(map(len, ds['train']['atoms']))).value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datasets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m oxid \u001b[38;5;241m=\u001b[39m \u001b[43mdatasets\u001b[49m\u001b[38;5;241m.\u001b[39mClassLabel(names\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m9\u001b[39m))))\n\u001b[1;32m      2\u001b[0m oxid\n",
      "\u001b[0;31mNameError\u001b[0m: name 'datasets' is not defined"
     ]
    }
   ],
   "source": [
    "oxid = datasets.ClassLabel(names=list(map(str, range(-5, 9))))\n",
    "oxid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "model = transformers.FlaxBertForTokenClassification(transformers.BertConfig(vocab_size=tok.get_vocab_size()))\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, split in ds.items():\n",
    "    split.to_json(root / f'{name}.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "from dataclasses import asdict, dataclass, field\n",
    "from enum import Enum\n",
    "from itertools import chain\n",
    "from pathlib import Path\n",
    "from typing import Any, Callable, Dict, Optional, Tuple\n",
    "\n",
    "import datasets\n",
    "import evaluate\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import optax\n",
    "from datasets import ClassLabel, load_dataset\n",
    "from flax import struct, traverse_util\n",
    "from flax.jax_utils import pad_shard_unpad, replicate, unreplicate\n",
    "from flax.training import train_state\n",
    "from flax.training.common_utils import get_metrics, onehot, shard\n",
    "from huggingface_hub import HfApi\n",
    "from tqdm import tqdm\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    FlaxAutoModelForTokenClassification,\n",
    "    HfArgumentParser,\n",
    "    is_tensorboard_available,\n",
    ")\n",
    "from transformers.utils import check_min_version, send_example_telemetry\n",
    "from transformers.utils.versions import require_version\n",
    "\n",
    "logger = logging.getLogger()\n",
    "\n",
    "Array = Any\n",
    "Dataset = datasets.arrow_dataset.Dataset\n",
    "PRNGKey = Any\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainingArguments:\n",
    "    output_dir: str = field(\n",
    "        metadata={\"help\": \"The output directory where the model predictions and checkpoints will be written.\"},\n",
    "    )\n",
    "    overwrite_output_dir: bool = field(\n",
    "        default=False,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"Overwrite the content of the output directory. \"\n",
    "                \"Use this to continue training if output_dir points to a checkpoint directory.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    do_train: bool = field(default=False, metadata={\"help\": \"Whether to run training.\"})\n",
    "    do_eval: bool = field(default=False, metadata={\"help\": \"Whether to run eval on the dev set.\"})\n",
    "    per_device_train_batch_size: int = field(\n",
    "        default=8, metadata={\"help\": \"Batch size per GPU/TPU core/CPU for training.\"}\n",
    "    )\n",
    "    per_device_eval_batch_size: int = field(\n",
    "        default=8, metadata={\"help\": \"Batch size per GPU/TPU core/CPU for evaluation.\"}\n",
    "    )\n",
    "    learning_rate: float = field(default=5e-5, metadata={\"help\": \"The initial learning rate for AdamW.\"})\n",
    "    weight_decay: float = field(default=0.0, metadata={\"help\": \"Weight decay for AdamW if we apply some.\"})\n",
    "    adam_beta1: float = field(default=0.9, metadata={\"help\": \"Beta1 for AdamW optimizer\"})\n",
    "    adam_beta2: float = field(default=0.999, metadata={\"help\": \"Beta2 for AdamW optimizer\"})\n",
    "    adam_epsilon: float = field(default=1e-8, metadata={\"help\": \"Epsilon for AdamW optimizer.\"})\n",
    "    adafactor: bool = field(default=False, metadata={\"help\": \"Whether or not to replace AdamW by Adafactor.\"})\n",
    "    num_train_epochs: float = field(default=3.0, metadata={\"help\": \"Total number of training epochs to perform.\"})\n",
    "    warmup_steps: int = field(default=0, metadata={\"help\": \"Linear warmup over warmup_steps.\"})\n",
    "    logging_steps: int = field(default=500, metadata={\"help\": \"Log every X updates steps.\"})\n",
    "    save_steps: int = field(default=500, metadata={\"help\": \"Save checkpoint every X updates steps.\"})\n",
    "    eval_steps: int = field(default=None, metadata={\"help\": \"Run an evaluation every X steps.\"})\n",
    "    seed: int = field(default=42, metadata={\"help\": \"Random seed that will be set at the beginning of training.\"})\n",
    "    push_to_hub: bool = field(\n",
    "        default=False, metadata={\"help\": \"Whether or not to upload the trained model to the model hub after training.\"}\n",
    "    )\n",
    "    hub_model_id: str = field(\n",
    "        default=None, metadata={\"help\": \"The name of the repository to keep in sync with the local `output_dir`.\"}\n",
    "    )\n",
    "    hub_token: str = field(default=None, metadata={\"help\": \"The token to use to push to the Model Hub.\"})\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.output_dir is not None:\n",
    "            self.output_dir = os.path.expanduser(self.output_dir)\n",
    "\n",
    "    def to_dict(self):\n",
    "        \"\"\"\n",
    "        Serializes this instance while replace `Enum` by their values (for JSON serialization support). It obfuscates\n",
    "        the token values by removing their value.\n",
    "        \"\"\"\n",
    "        d = asdict(self)\n",
    "        for k, v in d.items():\n",
    "            if isinstance(v, Enum):\n",
    "                d[k] = v.value\n",
    "            if isinstance(v, list) and len(v) > 0 and isinstance(v[0], Enum):\n",
    "                d[k] = [x.value for x in v]\n",
    "            if k.endswith(\"_token\"):\n",
    "                d[k] = f\"<{k.upper()}>\"\n",
    "        return d\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(output_dir='~/cdv/logs/bertos')\n",
    "\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n",
    "    \"\"\"\n",
    "\n",
    "    model_name_or_path: str = \"FlaxBertForTokenClassification\"\n",
    "    config = transformers.BertConfig(vocab_size=tok.get_vocab_size())\n",
    "    tokenizer = tok\n",
    "    cache_dir: Optional[str] = None\n",
    "    model_revision: str = \"main\"\n",
    "    token: str = None\n",
    "    use_auth_token: bool = None\n",
    "    trust_remote_code: bool = True\n",
    "\n",
    "\n",
    "model_args = ModelArguments()\n",
    "\n",
    "@dataclass\n",
    "class DataTrainingArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to what data we are going to input our model for training and eval.\n",
    "    \"\"\"\n",
    "\n",
    "    task_name: Optional[str] = field(default=\"ner\", metadata={\"help\": \"The name of the task (ner, pos...).\"})\n",
    "    train_file: Optional[str] = field(\n",
    "        default=root / \"train.json\", metadata={\"help\": \"The input training data file (a csv or JSON file).\"}\n",
    "    )\n",
    "    validation_file: Optional[str] = field(\n",
    "        default=root / \"valid.json\",\n",
    "        metadata={\"help\": \"An optional input evaluation data file to evaluate on (a csv or JSON file).\"},\n",
    "    )\n",
    "    test_file: Optional[str] = field(\n",
    "        default=root / \"test.json\",\n",
    "        metadata={\"help\": \"An optional input test data file to predict on (a csv or JSON file).\"},\n",
    "    )\n",
    "    text_column_name: Optional[str] = field(\n",
    "        default=\"atoms\", metadata={\"help\": \"The column name of text to input in the file (a csv or JSON file).\"}\n",
    "    )\n",
    "    label_column_name: Optional[str] = field(\n",
    "        default=\"labels\", metadata={\"help\": \"The column name of label to input in the file (a csv or JSON file).\"}\n",
    "    )\n",
    "    overwrite_cache: bool = field(\n",
    "        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n",
    "    )\n",
    "    preprocessing_num_workers: Optional[int] = field(\n",
    "        default=1,\n",
    "        metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n",
    "    )\n",
    "    max_seq_length: int = field(default=64)\n",
    "    max_train_samples: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n",
    "                \"value if set.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    max_eval_samples: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n",
    "                \"value if set.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    max_predict_samples: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"For debugging purposes or quicker training, truncate the number of prediction examples to this \"\n",
    "                \"value if set.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    label_all_tokens: bool = field(\n",
    "        default=True,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"Whether to put the label for one word on all tokens of generated by that word or just on the \"\n",
    "                \"one (in which case the other tokens will have a padding index).\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    return_entity_level_metrics: bool = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Whether to return all the entity levels during evaluation or just the overall ones.\"},\n",
    "    )\n",
    "\n",
    "    def __post_init__(self):\n",
    "        # if self.dataset_name is None and self.train_file is None and self.validation_file is None:\n",
    "        #     raise ValueError(\"Need either a dataset name or a training/validation file.\")\n",
    "        # else:\n",
    "        #     if self.train_file is not None:\n",
    "        #         extension = self.train_file.split(\".\")[-1]\n",
    "        #         assert extension in [\"csv\", \"json\"], \"`train_file` should be a csv or a json file.\"\n",
    "        #     if self.validation_file is not None:\n",
    "        #         extension = self.validation_file.split(\".\")[-1]\n",
    "        #         assert extension in [\"csv\", \"json\"], \"`validation_file` should be a csv or a json file.\"\n",
    "        self.task_name = self.task_name.lower()\n",
    "\n",
    "\n",
    "data_args = DataTrainingArguments()\n",
    "\n",
    "\n",
    "def create_train_state(\n",
    "    model: FlaxAutoModelForTokenClassification,\n",
    "    learning_rate_fn: Callable[[int], float],\n",
    "    num_labels: int,\n",
    "    training_args: TrainingArguments,\n",
    ") -> train_state.TrainState:\n",
    "    \"\"\"Create initial training state.\"\"\"\n",
    "\n",
    "    class TrainState(train_state.TrainState):\n",
    "        \"\"\"Train state with an Optax optimizer.\n",
    "\n",
    "        The two functions below differ depending on whether the task is classification\n",
    "        or regression.\n",
    "\n",
    "        Args:\n",
    "          logits_fn: Applied to last layer to obtain the logits.\n",
    "          loss_fn: Function to compute the loss.\n",
    "        \"\"\"\n",
    "\n",
    "        logits_fn: Callable = struct.field(pytree_node=False)\n",
    "        loss_fn: Callable = struct.field(pytree_node=False)\n",
    "\n",
    "    # We use Optax's \"masking\" functionality to not apply weight decay\n",
    "    # to bias and LayerNorm scale parameters. decay_mask_fn returns a\n",
    "    # mask boolean with the same structure as the parameters.\n",
    "    # The mask is True for parameters that should be decayed.\n",
    "    def decay_mask_fn(params):\n",
    "        flat_params = traverse_util.flatten_dict(params)\n",
    "        # find out all LayerNorm parameters\n",
    "        layer_norm_candidates = [\"layernorm\", \"layer_norm\", \"ln\"]\n",
    "        layer_norm_named_params = {\n",
    "            layer[-2:]\n",
    "            for layer_norm_name in layer_norm_candidates\n",
    "            for layer in flat_params.keys()\n",
    "            if layer_norm_name in \"\".join(layer).lower()\n",
    "        }\n",
    "        flat_mask = {path: (path[-1] != \"bias\" and path[-2:] not in layer_norm_named_params) for path in flat_params}\n",
    "        return traverse_util.unflatten_dict(flat_mask)\n",
    "\n",
    "    tx = optax.adamw(\n",
    "        learning_rate=learning_rate_fn,\n",
    "        b1=training_args.adam_beta1,\n",
    "        b2=training_args.adam_beta2,\n",
    "        eps=training_args.adam_epsilon,\n",
    "        weight_decay=training_args.weight_decay,\n",
    "        mask=decay_mask_fn,\n",
    "    )\n",
    "\n",
    "    def cross_entropy_loss(logits, labels):\n",
    "        xentropy = optax.softmax_cross_entropy(logits, onehot(labels, num_classes=num_labels))\n",
    "        return jnp.mean(xentropy)\n",
    "\n",
    "    return TrainState.create(\n",
    "        apply_fn=model.__call__,\n",
    "        params=model.params,\n",
    "        tx=tx,\n",
    "        logits_fn=lambda logits: logits.argmax(-1),\n",
    "        loss_fn=cross_entropy_loss,\n",
    "    )\n",
    "\n",
    "\n",
    "def create_learning_rate_fn(\n",
    "    train_ds_size: int, train_batch_size: int, num_train_epochs: int, num_warmup_steps: int, learning_rate: float\n",
    ") -> Callable[[int], jnp.ndarray]:\n",
    "    \"\"\"Returns a linear warmup, linear_decay learning rate function.\"\"\"\n",
    "    steps_per_epoch = train_ds_size // train_batch_size\n",
    "    num_train_steps = steps_per_epoch * num_train_epochs\n",
    "    warmup_fn = optax.linear_schedule(init_value=0.0, end_value=learning_rate, transition_steps=num_warmup_steps)\n",
    "    decay_fn = optax.linear_schedule(\n",
    "        init_value=learning_rate, end_value=0, transition_steps=num_train_steps - num_warmup_steps\n",
    "    )\n",
    "    schedule_fn = optax.join_schedules(schedules=[warmup_fn, decay_fn], boundaries=[num_warmup_steps])\n",
    "    return schedule_fn\n",
    "\n",
    "\n",
    "def train_data_collator(rng: PRNGKey, dataset: Dataset, batch_size: int):\n",
    "    \"\"\"Returns shuffled batches of size `batch_size` from truncated `train dataset`, sharded over all local devices.\"\"\"\n",
    "    steps_per_epoch = len(dataset) // batch_size\n",
    "    perms = jax.random.permutation(rng, len(dataset))\n",
    "    perms = perms[: steps_per_epoch * batch_size]  # Skip incomplete batch.\n",
    "    perms = perms.reshape((steps_per_epoch, batch_size))\n",
    "\n",
    "    for perm in perms:\n",
    "        batch = dataset[perm]\n",
    "        batch = {k: np.array(v) for k, v in batch.items()}\n",
    "        batch = shard(batch)\n",
    "\n",
    "        yield batch\n",
    "\n",
    "\n",
    "def eval_data_collator(dataset: Dataset, batch_size: int):\n",
    "    \"\"\"Returns batches of size `batch_size` from `eval dataset`. Sharding handled by `pad_shard_unpad` in the eval loop.\"\"\"\n",
    "    batch_idx = np.arange(len(dataset))\n",
    "\n",
    "    steps_per_epoch = math.ceil(len(dataset) / batch_size)\n",
    "    batch_idx = np.array_split(batch_idx, steps_per_epoch)\n",
    "\n",
    "    for idx in batch_idx:\n",
    "        batch = dataset[idx]\n",
    "        batch = {k: np.array(v) for k, v in batch.items()}\n",
    "\n",
    "        yield batch\n",
    "\n",
    "\n",
    "def main():\n",
    "    # See all possible arguments in src/transformers/training_args.py\n",
    "    # or by passing the --help flag to this script.\n",
    "    # We now keep distinct sets of args, for a cleaner separation of concerns.\n",
    "\n",
    "    # parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))\n",
    "    # if len(sys.argv) == 2 and sys.argv[1].endswith(\".json\"):\n",
    "    #     # If we pass only one argument to the script and it's the path to a json file,\n",
    "    #     # let's parse it to get our arguments.\n",
    "    #     model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))\n",
    "    # else:\n",
    "    #     model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
    "\n",
    "    # Make one log on every process with the configuration for debugging.\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO,\n",
    "    )\n",
    "    # Setup logging, we only want one process per machine to log things on the screen.\n",
    "    logger.setLevel(logging.INFO if jax.process_index() == 0 else logging.ERROR)\n",
    "    if jax.process_index() == 0:\n",
    "        datasets.utils.logging.set_verbosity_warning()\n",
    "        transformers.utils.logging.set_verbosity_info()\n",
    "    else:\n",
    "        datasets.utils.logging.set_verbosity_error()\n",
    "        transformers.utils.logging.set_verbosity_error()\n",
    "\n",
    "    # Get the datasets: you can either provide your own CSV/JSON/TXT training and evaluation files (see below)\n",
    "    # or just provide the name of one of the public datasets for token classification task available on the hub at https://huggingface.co/datasets/\n",
    "    # (the dataset will be downloaded automatically from the datasets Hub).\n",
    "    #\n",
    "    # For CSV/JSON files, this script will use the column called 'tokens' or the first column if no column called\n",
    "    # 'tokens' is found. You can easily tweak this behavior (see below).\n",
    "    #\n",
    "    # In distributed training, the load_dataset function guarantee that only one local process can concurrently\n",
    "    # download the dataset.\n",
    "    data_files = {}\n",
    "    if data_args.train_file is not None:\n",
    "        data_files[\"train\"] = str(data_args.train_file)\n",
    "    if data_args.validation_file is not None:\n",
    "        data_files[\"validation\"] = str(data_args.validation_file)\n",
    "    extension = str(data_args.train_file if data_args.train_file is not None else data_args.valid_file).split(\".\")[-1]\n",
    "    raw_datasets = load_dataset(\n",
    "        extension,\n",
    "        data_files=data_files,\n",
    "        cache_dir=model_args.cache_dir,\n",
    "        token=model_args.token,\n",
    "    )\n",
    "    # See more about loading any type of standard or custom dataset at\n",
    "    # https://huggingface.co/docs/datasets/loading_datasets.\n",
    "\n",
    "    if raw_datasets[\"train\"] is not None:\n",
    "        column_names = raw_datasets[\"train\"].column_names\n",
    "        features = raw_datasets[\"train\"].features\n",
    "    else:\n",
    "        column_names = raw_datasets[\"validation\"].column_names\n",
    "        features = raw_datasets[\"validation\"].features\n",
    "\n",
    "    if data_args.text_column_name is not None:\n",
    "        text_column_name = data_args.text_column_name\n",
    "    elif \"tokens\" in column_names:\n",
    "        text_column_name = \"tokens\"\n",
    "    else:\n",
    "        text_column_name = column_names[0]\n",
    "\n",
    "    if data_args.label_column_name is not None:\n",
    "        label_column_name = data_args.label_column_name\n",
    "    elif f\"{data_args.task_name}_tags\" in column_names:\n",
    "        label_column_name = f\"{data_args.task_name}_tags\"\n",
    "    else:\n",
    "        label_column_name = column_names[1]\n",
    "\n",
    "    # In the event the labels are not a `Sequence[ClassLabel]`, we will need to go through the dataset to get the\n",
    "    # unique labels.\n",
    "    def get_label_list(labels):\n",
    "        unique_labels = set()\n",
    "        for label in labels:\n",
    "            unique_labels = unique_labels | set(label)\n",
    "        label_list = list(unique_labels)\n",
    "        label_list.sort()\n",
    "        return label_list\n",
    "\n",
    "    if isinstance(features[label_column_name].feature, ClassLabel):\n",
    "        label_list = features[label_column_name].feature.names\n",
    "        # No need to convert the labels since they are already ints.\n",
    "        label_to_id = {i: i for i in range(len(label_list))}\n",
    "    else:\n",
    "        label_list = get_label_list(raw_datasets[\"train\"][label_column_name])\n",
    "        label_to_id = {l: i for i, l in enumerate(label_list)}\n",
    "    num_labels = len(label_list)\n",
    "\n",
    "    # Load pretrained model and tokenizer\n",
    "    config = transformers.BertConfig(\n",
    "        vocab_size=tok.get_vocab_size(),\n",
    "        num_labels=num_labels,\n",
    "        label2id=label_to_id,\n",
    "        id2label={i: l for l, i in label_to_id.items()},\n",
    "        finetuning_task=data_args.task_name,\n",
    "        cache_dir=model_args.cache_dir,\n",
    "        revision=model_args.model_revision,\n",
    "        token=model_args.token,\n",
    "        trust_remote_code=model_args.trust_remote_code)\n",
    "\n",
    "    # tokenizer_name_or_path = model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path\n",
    "    # if config.model_type in {\"gpt2\", \"roberta\"}:\n",
    "    #     tokenizer = AutoTokenizer.from_pretrained(\n",
    "    #         tokenizer_name_or_path,\n",
    "    #         cache_dir=model_args.cache_dir,\n",
    "    #         revision=model_args.model_revision,\n",
    "    #         token=model_args.token,\n",
    "    #         trust_remote_code=model_args.trust_remote_code,\n",
    "    #         add_prefix_space=True,\n",
    "    #     )\n",
    "    # else:\n",
    "    #     tokenizer = AutoTokenizer.from_pretrained(\n",
    "    #         tokenizer_name_or_path,\n",
    "    #         cache_dir=model_args.cache_dir,\n",
    "    #         revision=model_args.model_revision,\n",
    "    #         token=model_args.token,\n",
    "    #         trust_remote_code=model_args.trust_remote_code,\n",
    "    #     )\n",
    "\n",
    "    tokenizer = transformers.PreTrainedTokenizerFast(tokenizer_object=tok)\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    model = transformers.FlaxBertForTokenClassification(        \n",
    "        config=config,\n",
    "    )\n",
    "\n",
    "    # Preprocessing the datasets\n",
    "    # Tokenize all texts and align the labels with them.\n",
    "    def tokenize_and_align_labels(examples):\n",
    "        tokenized_inputs = tokenizer(\n",
    "            examples[text_column_name],\n",
    "            max_length=data_args.max_seq_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            # We use this argument because the texts in our dataset are lists of words (with a label for each word).\n",
    "            is_split_into_words=True,\n",
    "        )\n",
    "\n",
    "        labels = []\n",
    "\n",
    "        for i, label in enumerate(examples[label_column_name]):\n",
    "            word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "            previous_word_idx = None\n",
    "            label_ids = []\n",
    "            for word_idx in word_ids:\n",
    "                # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
    "                # ignored in the loss function.\n",
    "                if word_idx is None:\n",
    "                    label_ids.append(-100)\n",
    "                # We set the label for the first token of each word.\n",
    "                elif word_idx != previous_word_idx:\n",
    "                    label_ids.append(label_to_id[label[word_idx]])\n",
    "                # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "                # the label_all_tokens flag.\n",
    "                else:\n",
    "                    label_ids.append(label_to_id[label[word_idx]] if data_args.label_all_tokens else -100)\n",
    "                previous_word_idx = word_idx\n",
    "\n",
    "            labels.append(label_ids)\n",
    "        tokenized_inputs[\"labels\"] = labels\n",
    "        return tokenized_inputs\n",
    "\n",
    "    processed_raw_datasets = raw_datasets.map(\n",
    "        tokenize_and_align_labels,\n",
    "        batched=True,\n",
    "        num_proc=data_args.preprocessing_num_workers,\n",
    "        load_from_cache_file=not data_args.overwrite_cache,\n",
    "        remove_columns=raw_datasets[\"train\"].column_names,\n",
    "        desc=\"Running tokenizer on dataset\",\n",
    "    )\n",
    "\n",
    "    train_dataset = processed_raw_datasets[\"train\"]\n",
    "    eval_dataset = processed_raw_datasets[\"validation\"]\n",
    "\n",
    "    # Log a few random samples from the training set:\n",
    "    for index in random.sample(range(len(train_dataset)), 3):\n",
    "        logger.info(f\"Sample {index} of the training set: {train_dataset[index]}.\")\n",
    "\n",
    "    # Define a summary writer\n",
    "    has_tensorboard = is_tensorboard_available()\n",
    "    if has_tensorboard and jax.process_index() == 0:\n",
    "        try:\n",
    "            from flax.metrics.tensorboard import SummaryWriter\n",
    "\n",
    "            summary_writer = SummaryWriter(training_args.output_dir)\n",
    "            summary_writer.hparams({**training_args.to_dict(), **vars(model_args), **vars(data_args)})\n",
    "        except ImportError as ie:\n",
    "            has_tensorboard = False\n",
    "            logger.warning(\n",
    "                f\"Unable to display metrics through TensorBoard because some package are not installed: {ie}\"\n",
    "            )\n",
    "    else:\n",
    "        logger.warning(\n",
    "            \"Unable to display metrics through TensorBoard because the package is not installed: \"\n",
    "            \"Please run pip install tensorboard to enable.\"\n",
    "        )\n",
    "\n",
    "    def write_train_metric(summary_writer, train_metrics, train_time, step):\n",
    "        summary_writer.scalar(\"train_time\", train_time, step)\n",
    "\n",
    "        train_metrics = get_metrics(train_metrics)\n",
    "        for key, vals in train_metrics.items():\n",
    "            tag = f\"train_{key}\"\n",
    "            for i, val in enumerate(vals):\n",
    "                summary_writer.scalar(tag, val, step - len(vals) + i + 1)\n",
    "\n",
    "    def write_eval_metric(summary_writer, eval_metrics, step):\n",
    "        for metric_name, value in eval_metrics.items():\n",
    "            summary_writer.scalar(f\"eval_{metric_name}\", value, step)\n",
    "\n",
    "    num_epochs = int(training_args.num_train_epochs)\n",
    "    rng = jax.random.PRNGKey(training_args.seed)\n",
    "    dropout_rngs = jax.random.split(rng, jax.local_device_count())\n",
    "\n",
    "    train_batch_size = training_args.per_device_train_batch_size * jax.local_device_count()\n",
    "    per_device_eval_batch_size = int(training_args.per_device_eval_batch_size)\n",
    "    eval_batch_size = training_args.per_device_eval_batch_size * jax.local_device_count()\n",
    "\n",
    "    learning_rate_fn = create_learning_rate_fn(\n",
    "        len(train_dataset),\n",
    "        train_batch_size,\n",
    "        training_args.num_train_epochs,\n",
    "        training_args.warmup_steps,\n",
    "        training_args.learning_rate,\n",
    "    )\n",
    "\n",
    "    state = create_train_state(model, learning_rate_fn, num_labels=num_labels, training_args=training_args)\n",
    "\n",
    "    # define step functions\n",
    "    def train_step(\n",
    "        state: train_state.TrainState, batch: Dict[str, Array], dropout_rng: PRNGKey\n",
    "    ) -> Tuple[train_state.TrainState, float]:\n",
    "        \"\"\"Trains model with an optimizer (both in `state`) on `batch`, returning a pair `(new_state, loss)`.\"\"\"\n",
    "        dropout_rng, new_dropout_rng = jax.random.split(dropout_rng)\n",
    "        targets = batch.pop(\"labels\")\n",
    "\n",
    "        def loss_fn(params):\n",
    "            logits = state.apply_fn(**batch, params=params, dropout_rng=dropout_rng, train=True)[0]\n",
    "            loss = state.loss_fn(logits, targets)\n",
    "            return loss\n",
    "\n",
    "        grad_fn = jax.value_and_grad(loss_fn)\n",
    "        loss, grad = grad_fn(state.params)\n",
    "        grad = jax.lax.pmean(grad, \"batch\")\n",
    "        new_state = state.apply_gradients(grads=grad)\n",
    "        metrics = jax.lax.pmean({\"loss\": loss, \"learning_rate\": learning_rate_fn(state.step)}, axis_name=\"batch\")\n",
    "        return new_state, metrics, new_dropout_rng\n",
    "\n",
    "    p_train_step = jax.pmap(train_step, axis_name=\"batch\", donate_argnums=(0,))\n",
    "\n",
    "    def eval_step(state, batch):\n",
    "        logits = state.apply_fn(**batch, params=state.params, train=False)[0]\n",
    "        return state.logits_fn(logits)\n",
    "\n",
    "    p_eval_step = jax.pmap(eval_step, axis_name=\"batch\")\n",
    "\n",
    "    metric = evaluate.load(\"seqeval\", cache_dir=model_args.cache_dir)\n",
    "\n",
    "    def get_labels(y_pred, y_true):\n",
    "        # Transform predictions and references tensos to numpy arrays\n",
    "\n",
    "        # Remove ignored index (special tokens)\n",
    "        true_predictions = [\n",
    "            [label_list[p] for (p, l) in zip(pred, gold_label) if l != -100]\n",
    "            for pred, gold_label in zip(y_pred, y_true)\n",
    "        ]\n",
    "        true_labels = [\n",
    "            [label_list[l] for (p, l) in zip(pred, gold_label) if l != -100]\n",
    "            for pred, gold_label in zip(y_pred, y_true)\n",
    "        ]\n",
    "        return true_predictions, true_labels\n",
    "\n",
    "    def compute_metrics():\n",
    "        results = metric.compute()\n",
    "        if data_args.return_entity_level_metrics:\n",
    "            # Unpack nested dictionaries\n",
    "            final_results = {}\n",
    "            for key, value in results.items():\n",
    "                if isinstance(value, dict):\n",
    "                    for n, v in value.items():\n",
    "                        final_results[f\"{key}_{n}\"] = v\n",
    "                else:\n",
    "                    final_results[key] = value\n",
    "            return final_results\n",
    "        else:\n",
    "            return {\n",
    "                \"precision\": results[\"overall_precision\"],\n",
    "                \"recall\": results[\"overall_recall\"],\n",
    "                \"f1\": results[\"overall_f1\"],\n",
    "                \"accuracy\": results[\"overall_accuracy\"],\n",
    "            }\n",
    "\n",
    "    logger.info(f\"===== Starting training ({num_epochs} epochs) =====\")\n",
    "    train_time = 0\n",
    "\n",
    "    # make sure weights are replicated on each device\n",
    "    state = replicate(state)\n",
    "\n",
    "    train_time = 0\n",
    "    step_per_epoch = len(train_dataset) // train_batch_size\n",
    "    total_steps = step_per_epoch * num_epochs\n",
    "    epochs = tqdm(range(num_epochs), desc=f\"Epoch ... (1/{num_epochs})\", position=0)\n",
    "    for epoch in epochs:\n",
    "        train_start = time.time()\n",
    "        train_metrics = []\n",
    "\n",
    "        # Create sampling rng\n",
    "        rng, input_rng = jax.random.split(rng)\n",
    "\n",
    "        # train\n",
    "        for step, batch in enumerate(\n",
    "            tqdm(\n",
    "                train_data_collator(input_rng, train_dataset, train_batch_size),\n",
    "                total=step_per_epoch,\n",
    "                desc=\"Training...\",\n",
    "                position=1,\n",
    "            )\n",
    "        ):\n",
    "            state, train_metric, dropout_rngs = p_train_step(state, batch, dropout_rngs)\n",
    "            train_metrics.append(train_metric)\n",
    "\n",
    "            cur_step = (epoch * step_per_epoch) + (step + 1)\n",
    "\n",
    "            if cur_step % training_args.logging_steps == 0 and cur_step > 0:\n",
    "                # Save metrics\n",
    "                train_metric = unreplicate(train_metric)\n",
    "                train_time += time.time() - train_start\n",
    "                if has_tensorboard and jax.process_index() == 0:\n",
    "                    write_train_metric(summary_writer, train_metrics, train_time, cur_step)\n",
    "\n",
    "                epochs.write(\n",
    "                    f\"Step... ({cur_step}/{total_steps} | Training Loss: {train_metric['loss']}, Learning Rate:\"\n",
    "                    f\" {train_metric['learning_rate']})\"\n",
    "                )\n",
    "\n",
    "                train_metrics = []\n",
    "\n",
    "            if cur_step % training_args.eval_steps == 0 and cur_step > 0:\n",
    "                eval_metrics = {}\n",
    "                # evaluate\n",
    "                for batch in tqdm(\n",
    "                    eval_data_collator(eval_dataset, eval_batch_size),\n",
    "                    total=math.ceil(len(eval_dataset) / eval_batch_size),\n",
    "                    desc=\"Evaluating ...\",\n",
    "                    position=2,\n",
    "                ):\n",
    "                    labels = batch.pop(\"labels\")\n",
    "                    predictions = pad_shard_unpad(p_eval_step)(\n",
    "                        state, batch, min_device_batch=per_device_eval_batch_size\n",
    "                    )\n",
    "                    predictions = np.array(predictions)\n",
    "                    labels[np.array(chain(*batch[\"attention_mask\"])) == 0] = -100\n",
    "                    preds, refs = get_labels(predictions, labels)\n",
    "                    metric.add_batch(\n",
    "                        predictions=preds,\n",
    "                        references=refs,\n",
    "                    )\n",
    "\n",
    "                eval_metrics = compute_metrics()\n",
    "\n",
    "                if data_args.return_entity_level_metrics:\n",
    "                    logger.info(f\"Step... ({cur_step}/{total_steps} | Validation metrics: {eval_metrics}\")\n",
    "                else:\n",
    "                    logger.info(\n",
    "                        f\"Step... ({cur_step}/{total_steps} | Validation f1: {eval_metrics['f1']}, Validation Acc:\"\n",
    "                        f\" {eval_metrics['accuracy']})\"\n",
    "                    )\n",
    "\n",
    "                if has_tensorboard and jax.process_index() == 0:\n",
    "                    write_eval_metric(summary_writer, eval_metrics, cur_step)\n",
    "\n",
    "            if (cur_step % training_args.save_steps == 0 and cur_step > 0) or (cur_step == total_steps):\n",
    "                # save checkpoint after each epoch and push checkpoint to the hub\n",
    "                if jax.process_index() == 0:\n",
    "                    params = jax.device_get(unreplicate(state.params))\n",
    "                    model.save_pretrained(training_args.output_dir, params=params)\n",
    "                    tokenizer.save_pretrained(training_args.output_dir)\n",
    "        epochs.desc = f\"Epoch ... {epoch + 1}/{num_epochs}\"\n",
    "\n",
    "    # Eval after training\n",
    "    if training_args.do_eval:\n",
    "        eval_metrics = {}\n",
    "        eval_loader = eval_data_collator(eval_dataset, eval_batch_size)\n",
    "        for batch in tqdm(eval_loader, total=len(eval_dataset) // eval_batch_size, desc=\"Evaluating ...\", position=2):\n",
    "            labels = batch.pop(\"labels\")\n",
    "            predictions = pad_shard_unpad(p_eval_step)(state, batch, min_device_batch=per_device_eval_batch_size)\n",
    "            predictions = np.array(predictions)\n",
    "            labels[np.array(chain(*batch[\"attention_mask\"])) == 0] = -100\n",
    "            preds, refs = get_labels(predictions, labels)\n",
    "            metric.add_batch(predictions=preds, references=refs)\n",
    "\n",
    "        eval_metrics = compute_metrics()\n",
    "\n",
    "        if jax.process_index() == 0:\n",
    "            eval_metrics = {f\"eval_{metric_name}\": value for metric_name, value in eval_metrics.items()}\n",
    "            path = os.path.join(training_args.output_dir, \"eval_results.json\")\n",
    "            with open(path, \"w\") as f:\n",
    "                json.dump(eval_metrics, f, indent=4, sort_keys=True)\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "avid",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
