diff --git a/cdv/config.py b/cdv/config.py
index f585d31..dc32cf2 100644
--- a/cdv/config.py
+++ b/cdv/config.py
@@ -18,12 +18,19 @@ from pyrallis.fields import field
 from cdv import layers
 from cdv.layers import E3Irreps, Identity, LazyInMLP
 from cdv.mace.e3_layers import LinearReadoutBlock, NonlinearReadoutBlock
-from cdv.mace.edge_embedding import PolynomialCutoff, RadialEmbeddingBlock, GaussBasis, ExpCutoff
+from cdv.mace.edge_embedding import (
+    BesselBasis,
+    PolynomialCutoff,
+    RadialEmbeddingBlock,
+    OldBessel1DBasis,
+    GaussBasis,
+    ExpCutoff,
+)
 from cdv.mace.mace import (
     MaceModel,
 )
 from cdv.mace.message_passing import InteractionBlock, MessagePassingConvolution
-from cdv.mace.node_embedding import SevenNetEmbedding
+from cdv.mace.node_embedding import LinearNodeEmbedding, SevenNetEmbedding
 from cdv.mace.self_connection import LinearSelfConnection, MLPSelfGate
 from cdv.regression import EFSLoss
 from cdv.vae import VAE, Decoder, Encoder, LatticeVAE, PropertyPredictor
@@ -38,7 +45,6 @@ class LogConfig:
     exp_name: Optional[str] = None
 
     # How many times to make a log each epoch.
-    # 208 = 2^4 * 13 steps per epoch with batch of 1: evenly dividing this is nice.
     logs_per_epoch: int = 8
 
     # Checkpoint every n epochs:
@@ -327,7 +333,8 @@ class MACEConfig:
     avg_num_neighbors: float = 14
     max_ell: int = 3
     hidden_irreps: tuple[str, ...] = ('128x0e + 64x1o + 32x2e', '128x0e + 64x1o + 32x2e')
-    node_out_dim: int = 64
+    species_embed_dim: int = 64
+    outs_per_node: int = 64
     head: MLPConfig = field(default_factory=MLPConfig)
     interaction_reduction: str = 'last'
     share_species_embed: bool = True
@@ -339,9 +346,14 @@ class MACEConfig:
     ) -> MaceModel:
         return MaceModel(
             hidden_irreps=self.hidden_irreps,
-            node_embedding=SevenNetEmbedding('32x0e'),
+            # node_embedding=SevenNetEmbedding('32x0e'),
+            node_embedding=LinearNodeEmbedding(
+                f'{self.species_embed_dim}x0e',
+                num_species=num_species,
+                element_indices=jnp.array(elem_indices),
+            ),
             edge_embedding=RadialEmbeddingBlock(
-                basis=GaussBasis(self.num_basis, r_max=self.r_max, sd=1),
+                basis=OldBessel1DBasis(self.num_basis, r_max=self.r_max),
                 envelope=ExpCutoff(r_max=self.r_max, c=1, cutoff_start=0.6),
                 # envelope=PolynomialCutoff(r_max=self.r_max, p=6),
             ),
@@ -349,15 +361,14 @@ class MACEConfig:
                 irreps_out=None,
                 conv=MessagePassingConvolution(
                     avg_num_neighbors=self.avg_num_neighbors,
-                    target_irreps=str(E3Irreps.spherical_harmonics(self.max_ell)),
                     max_ell=self.max_ell,
                 ),
             ),
             readout=LinearReadoutBlock(irreps_out=None),
             head_templ=self.head.build(),
-            # self_connection=MLPSelfGate(irreps_out=None),
-            self_connection=LinearSelfConnection(irreps_out=None),
-            node_out_dim=self.node_out_dim,
+            self_connection=MLPSelfGate(irreps_out=None),
+            # self_connection=LinearSelfConnection(irreps_out=None),
+            outs_per_node=self.outs_per_node,
             share_species_embed=self.share_species_embed,
             interaction_reduction=self.interaction_reduction,
         )
@@ -481,6 +492,9 @@ class MainConfig:
     # Folder to initialize the encoders and downsampling.
     encoder_start_from: Optional[Path] = None
 
+    # Debug mode: turns off mid-run checkpointing and Neptune tracking.
+    debug_mode: bool = False
+
     # Display kind for training runs: One of 'dashboard', 'progress', or 'quiet'.
     display: str = 'dashboard'
 
@@ -556,6 +570,29 @@ class MainConfig:
         else:
             raise ValueError(f'{self.regressor} not supported')
 
+    def as_dict(self):
+        """Serializes the relevant values into a dictionary suitable for e.g., Neptune logging."""
+        cfg: dict = pyrallis.encode(self)
+        for key in ('do_profile', 'cli', 'log', 'debug_mode', 'display'):
+            cfg.pop(key)
+
+        for key in ('raw_data_folder', 'data_folder', 'batch_n_nodes', 'batch_n_graphs'):
+            cfg['data'].pop(key)
+
+        def convert_leaves(leaf):
+            if isinstance(leaf, dict):
+                return {k: convert_leaves(v) for k, v in leaf.items()}
+            elif leaf is None:
+                return 'None'
+            elif isinstance(leaf, (tuple, list)):
+                return {i: convert_leaves(v) for i, v in enumerate(leaf)}
+            else:
+                return leaf
+
+        cfg = convert_leaves(cfg)
+
+        return cfg
+
 
 if __name__ == '__main__':
     from pathlib import Path
@@ -575,4 +612,6 @@ if __name__ == '__main__':
             pyrallis.cfgparsing.dump(default, outfile, omit_defaults=True)
 
         with default_path.open('r') as conf:
-            pyrallis.cfgparsing.load(MainConfig, conf)
+            cfg = pyrallis.cfgparsing.load(MainConfig, conf)
+
+        print(cfg.as_dict())
diff --git a/cdv/mace/edge_embedding.py b/cdv/mace/edge_embedding.py
index 5fa724b..2f34bb9 100644
--- a/cdv/mace/edge_embedding.py
+++ b/cdv/mace/edge_embedding.py
@@ -5,7 +5,7 @@ import jax.numpy as jnp
 from flax import linen as nn
 from jaxtyping import Float, Array
 
-from cdv.layers import Context, E3IrrepsArray
+from cdv.layers import Context, E3IrrepsArray, Identity
 
 
 class RadialBasis(nn.Module):
@@ -71,6 +71,40 @@ class BesselBasis(RadialBasis):
         return self.prefactor * (numerator / (edge_lengths[..., None] + 1e-4))
 
 
+class OldBessel1DBasis(RadialBasis):
+    """Uses spherical Bessel functions with a cutoff, as in DimeNet++."""
+
+    r_max: float
+    freq_trainable: bool = True
+
+    def setup(self):
+        def freq_init(rng):
+            return jnp.arange(self.num_basis, dtype=jnp.float32) + 1
+
+        if self.freq_trainable:
+            self.freq = self.param('freq', freq_init)
+        else:
+            self.freq = freq_init(None)
+
+    def __call__(self, x, ctx: Context):
+        dist = x[..., None] / self.r_max
+
+        # e(d) = sqrt(2/c) * sin(fπd/c)/d
+        # we use sinc so it's defined at 0
+        # jnp.sinc is sin(πx)/(πx)
+        # e(d) = sqrt(2/c) * sin(πfd/c)/(fd/c) * f/c
+        # e(d) = sqrt(2/c) * sinc(πfd/c)) * πf/c
+
+        e_d = (
+            jnp.sqrt(2 / self.r_max)
+            * jnp.sinc(self.freq * dist)
+            * (jnp.pi * self.freq / self.r_max)
+        )
+
+        # debug_stat(e_d=e_d, env=env, dist=dist)
+        return e_d
+
+
 class PolynomialCutoff(Envelope):
     # https://github.com/ACEsuit/mace/blob/575af0171369e2c19e04b115140b9901f83eb00c/mace/modules/radial.py#L112
 
@@ -113,3 +147,35 @@ class ExpCutoff(Envelope):
         envelope = 1 - jnp.where(t < 0.5, exp_func(2 * t) / 2, 1 - exp_func(2 - 2 * t) / 2)
 
         return envelope
+
+
+if __name__ == '__main__':
+    import jax
+    import jax.random as jr
+    from cdv.utils import debug_stat, debug_structure
+
+    rng = jr.key(123)
+    radii = jr.truncated_normal(rng, lower=-3.4, upper=5, shape=(32, 16), dtype=jnp.bfloat16) + 4
+    data = {}
+    kwargs = dict(num_basis=10, r_max=7)
+    cutoff = ExpCutoff(r_max=kwargs['r_max'], c=1, cutoff_start=0.6)
+
+    mods = [cutoff]
+    for basis in (GaussBasis(**kwargs), BesselBasis(**kwargs), OldBessel1DBasis(**kwargs)):
+        mods.append(RadialEmbeddingBlock(basis=basis, envelope=cutoff))
+
+    for mod in mods:
+        name = mod.basis.__class__.__name__ if hasattr(mod, 'basis') else 'raw'
+
+        def embed(radii):
+            out, params = mod.init_with_output(rng, radii, ctx=Context(training=True))
+            if hasattr(out, 'array'):
+                return out.array
+            else:
+                return out
+
+        data[name] = embed(radii)
+        data[name + '_grad'] = jax.grad(lambda x: jnp.sum(embed(x)))(radii)
+
+    # debug_structure(**data)
+    debug_stat(**data)
diff --git a/cdv/mace/mace.py b/cdv/mace/mace.py
index ef51807..5df5620 100644
--- a/cdv/mace/mace.py
+++ b/cdv/mace/mace.py
@@ -164,7 +164,7 @@ class MaceModel(nn.Module):
     readout: IrrepsModule
     head_templ: LazyInMLP
 
-    node_out_dim: int
+    outs_per_node: int
     share_species_embed: bool = True
 
     # How to combine the outputs of different interaction blocks.
@@ -173,7 +173,7 @@ class MaceModel(nn.Module):
 
     def setup(self):
         self.mace = MACE(
-            irreps_out=f'{self.node_out_dim}x0e',
+            irreps_out=f'{self.outs_per_node}x0e',
             hidden_irreps=self.hidden_irreps,
             node_embedding=self.node_embedding,
             radial_embedding=self.edge_embedding,
diff --git a/cdv/mace/message_passing.py b/cdv/mace/message_passing.py
index 8262bef..03fe3b3 100644
--- a/cdv/mace/message_passing.py
+++ b/cdv/mace/message_passing.py
@@ -4,11 +4,11 @@ from cdv.mace.e3_layers import E3Irreps, E3IrrepsArray, IrrepsModule, Linear
 import jax.numpy as jnp
 import e3nn_jax as e3nn
 from cdv.layers import Context, LazyInMLP
+from cdv.utils import debug_structure
 
 
 class MessagePassingConvolution(nn.Module):
     avg_num_neighbors: float
-    target_irreps: E3Irreps
     max_ell: int
 
     @nn.compact
@@ -28,11 +28,13 @@ class MessagePassingConvolution(nn.Module):
         ]
         # debug_structure(msgs=messages, vecs=vectors)
 
-        msg_prefix = messages_broadcast.filter(self.target_irreps)
+        inner_irreps = e3nn.Irreps.spherical_harmonics(self.max_ell)
+
+        msg_prefix = messages_broadcast.filter(inner_irreps)
         vec_harms = e3nn.tensor_product(
             messages_broadcast,
             e3nn.spherical_harmonics(range(1, self.max_ell + 1), vectors, True),
-            filter_ir_out=self.target_irreps,
+            filter_ir_out=inner_irreps,
         )
 
         # debug_structure(
@@ -62,6 +64,7 @@ class MessagePassingConvolution(nn.Module):
         zeros = E3IrrepsArray.zeros(messages.irreps, node_feats.shape[:1], messages.dtype)
         # TODO flip this perhaps?
         node_feats = zeros.at[receivers].add(messages)  # [n_nodes, irreps]
+        node_feats = node_feats / self.avg_num_neighbors
 
         return node_feats
 
@@ -82,9 +85,10 @@ class InteractionBlock(IrrepsModule):
         # assert node_feats.ndim == 2
         # assert vectors.ndim == 2
         # assert radial_embedding.ndim == 2
-        new_node_feats = Linear(node_feats.irreps, name='linear_up')(node_feats)
+        # new_node_feats = Linear(node_feats.irreps, name='linear_up')(node_feats)
+        new_node_feats = node_feats
         new_node_feats = self.conv(vectors, new_node_feats, radial_embedding, receivers, ctx)
-        new_node_feats = Linear(self.conv.target_irreps, name='linear_down')(new_node_feats)
+        new_node_feats = Linear(self.ir_out, name='linear_down')(new_node_feats)
 
         if new_node_feats.irreps == node_feats.irreps:
             node_feats = new_node_feats + node_feats
diff --git a/cdv/mace/node_embedding.py b/cdv/mace/node_embedding.py
index bcb0cdf..814351c 100644
--- a/cdv/mace/node_embedding.py
+++ b/cdv/mace/node_embedding.py
@@ -29,8 +29,8 @@ class LinearNodeEmbedding(NodeEmbedding):
 
     def setup(self):
         if self.ir_out.lmax > 0:
-            raise ValueError(f'Irreps {self.irreps_out} should just be scalars for node embedding.')
-        self.out_dim = self.irreps_out.dim
+            raise ValueError(f'Irreps {self.ir_out} should just be scalars for node embedding.')
+        self.out_dim = self.ir_out.dim
         self.embed = nn.Embed(self.num_species, self.out_dim)
 
     def __call__(self, node_species: Int[Array, ' nodes'], ctx: Context) -> E3IrrepsArray:
diff --git a/cdv/mace/self_connection.py b/cdv/mace/self_connection.py
index 19e31fc..dba77d1 100644
--- a/cdv/mace/self_connection.py
+++ b/cdv/mace/self_connection.py
@@ -265,12 +265,19 @@ class MLPSelfGate(SelfConnectionBlock):
         species_embed: Float[Array, 'num_species embed_dim'],
         ctx: Context,
     ):
-        scalars = e3nn.tensor_product(node_feats, node_feats, filter_ir_out=['0e'])
-        x = jnp.concat((scalars.array, species_embed), axis=-1)
-        out_dim = node_feats.filter(drop='0e').irreps.num_irreps
-        mlp = LazyInMLP(inner_dims=[out_dim // 2], out_dim=out_dim, name='self_gate')
-        y = E3IrrepsArray(f'{out_dim}x0e', mlp(x, ctx=ctx))
-        z = e3nn.concatenate((node_feats, y))
-        z = e3nn.gate(z)
-
-        return Linear(self.ir_out)(z)
+        if node_feats.irreps.lmax == 0:
+            # all scalars: no need to gate, just straight MLP
+            size = node_feats.irreps.num_irreps
+            mlp = LazyInMLP(inner_dims=[size], out_dim=size, name='self_mlp')
+            pre_out = mlp(node_feats.array, ctx=ctx)
+        else:
+            scalars = e3nn.tensor_product(node_feats, node_feats, filter_ir_out=['0e'])
+            x = jnp.concat((scalars.array, species_embed), axis=-1)
+            out_dim = node_feats.filter(drop='0e').irreps.num_irreps
+            mlp = LazyInMLP(inner_dims=[out_dim // 2], out_dim=out_dim, name='self_gate')
+            y = E3IrrepsArray(f'{out_dim}x0e', mlp(x, ctx=ctx))
+            z = e3nn.concatenate((node_feats, y))
+            z = e3nn.gate(z)
+            pre_out = z
+
+        return Linear(self.ir_out)(pre_out)
diff --git a/cdv/show_model.py b/cdv/show_model.py
index a5a9179..97c009c 100644
--- a/cdv/show_model.py
+++ b/cdv/show_model.py
@@ -18,8 +18,7 @@ def to_dot_graph(x):
     return xla_client._xla.hlo_module_to_dot_graph(xla_client._xla.hlo_module_from_text(x))
 
 
-@pyrallis.argparsing.wrap()
-def show_model(config: MainConfig, make_hlo_dot=False, do_profile=False):
+def show_model(config: MainConfig, make_hlo_dot=False, do_profile=False, show_stat=False):
     # print(config.data.avg_dist(6), config.data.avg_num_neighbors(6))
     kwargs = dict(ctx=Context(training=True))
     num_batches, dl = dataloader(config, split='train', infinite=True)
@@ -28,7 +27,7 @@ def show_model(config: MainConfig, make_hlo_dot=False, do_profile=False):
 
     batch = jax.device_put(batch, config.device.devices()[0])
 
-    debug_structure(batch=batch)
+    # debug_structure(batch=batch)
 
     if config.task == 'e_form':
         mod = config.build_regressor()
@@ -69,16 +68,15 @@ def show_model(config: MainConfig, make_hlo_dot=False, do_profile=False):
     debug_structure(module=mod, out=out, loss=loss)
     debug_stat(input=batch, out=out, loss=loss)
     rngs.pop('params')
-    flax_summary(mod, rngs=rngs, cg=b1, **kwargs)
 
     rot_batch, rots = jax.vmap(lambda x: x.rotate(123))(batch)
-    debug_structure(rots=rots)
+    # debug_structure(rots=rots)
 
-    if True:
-        rot_out = apply_fn(params, rot_batch)
-    else:
+    if show_stat:
         with nn.intercept_methods(intercept_stat):
             rot_out = base_apply_fn(params, rot_batch)
+    else:
+        rot_out = apply_fn(params, rot_batch)
 
     debug_stat(
         equiv_error=jax.tree.map(
@@ -88,16 +86,29 @@ def show_model(config: MainConfig, make_hlo_dot=False, do_profile=False):
         )
     )
 
+    flax_summary(mod, rngs=rngs, cg=b1, **kwargs)
+
+    val_and_grad = jax.jit(jax.value_and_grad(lambda x: jnp.mean(loss_fn(x)['loss'])))
+    cost_analysis = val_and_grad.lower(params).cost_analysis()
+    if cost_analysis is not None and 'flops' in cost_analysis:
+        cost = cost_analysis['flops']
+    else:
+        cost = 0
+
+    cost /= 1e9
+
+    print(f'Total cost: {cost:.3f} GFLOPs')
+
     if do_profile:
         with jax.profiler.trace('/tmp/jax-trace', create_perfetto_trace=True):
-            val, grad = jax.value_and_grad(lambda x: jnp.mean(loss_fn(x)['loss']))(params)
+            val, grad = val_and_grad(params)
             jax.block_until_ready(grad)
     else:
-        val, grad = jax.value_and_grad(lambda x: jnp.mean(loss_fn(x)['loss']))(params)
-    debug_stat(val=val, grad=grad)
+        val, grad = val_and_grad(params)
+    debug_stat(loss=val, grad=grad, tree_depth=6)
 
     if not make_hlo_dot:
-        return
+        return cost
 
     grad_loss = jax.xla_computation(jax.value_and_grad(loss))(params)
     with open('model.hlo', 'w') as f:
@@ -116,6 +127,8 @@ def show_model(config: MainConfig, make_hlo_dot=False, do_profile=False):
     for f in ('model.dot', 'model_opt.dot'):
         subprocess.run(['sfdp', f, '-Tsvg', '-O', '-x'])
 
+    return cost
+
 
 if __name__ == '__main__':
-    show_model()
+    pyrallis.argparsing.wrap()(show_model)()
diff --git a/cdv/training_state.py b/cdv/training_state.py
index cc837dc..f7e7a86 100644
--- a/cdv/training_state.py
+++ b/cdv/training_state.py
@@ -7,7 +7,7 @@ from datetime import datetime, timedelta
 from os import PathLike
 from pathlib import Path
 from shutil import copytree
-from typing import Any, Mapping, Sequence
+from typing import Any, Literal, Mapping, Sequence
 
 import chex
 import jax
@@ -24,9 +24,13 @@ from cdv.checkpointing import best_ckpt
 from cdv.config import LossConfig, MainConfig
 from cdv.dataset import CrystalGraphs, dataloader
 from cdv.layers import Context
+from cdv.model_summary import model_summary
 from cdv.regression import EFSOutput, EFSWrapper
 from cdv.utils import item_if_arr
 
+import neptune
+from neptune.types import File
+
 
 @struct.dataclass
 class Metrics:
@@ -117,8 +121,30 @@ class TrainingRun:
             options=opts,
         )
 
+        self.run = self.init_run()
+
         self.test_loss = 1000
 
+    def init_run(self) -> neptune.Run:
+        run = neptune.init_run(
+            mode='debug' if self.config.debug_mode else 'async',
+            tags=[self.config.task],
+            capture_stderr=False,
+            capture_stdout=False,
+            source_files='cdv/**/*.py',
+        )
+
+        run['parameters'] = self.config.as_dict()
+        run['full_config'] = File.from_content(
+            pyrallis.cfgparsing.dump(self.config, omit_defaults=False), extension='toml'
+        )
+        summary = model_summary(self.config)
+        run['model_summary'] = File.from_content(summary['html'], extension='html')
+        run['gflops'] = summary['gflops']
+        run['dataset_metadata'].track_files(str(self.config.data.dataset_folder / 'metadata.json'))
+        run['seed'] = self.seed
+        return run
+
     @staticmethod
     @ft.partial(jax.jit, static_argnames=('task', 'config'))
     @chex.assert_max_traces(5)
@@ -228,6 +254,19 @@ class TrainingRun:
     def next_step(self):
         return self.step(self.curr_step + 1, next(self.dl))
 
+    def log_metric(self, metric_name: str, metric_value, split: Literal['train', 'valid', None]):
+        if split == 'train':
+            self.metrics_history[f'tr_{metric_name}'].append(metric_value)
+            self.run[f'train/{metric_name}'].append(value=metric_value, step=self.curr_epoch)
+        elif split == 'valid':
+            self.metrics_history[f'te_{metric_name}'].append(metric_value)
+            self.run[f'valid/{metric_name}'].append(value=metric_value, step=self.curr_epoch)
+        elif split is None:
+            self.metrics_history[f'{metric_name}'].append(metric_value)
+            self.run[f'{metric_name}'].append(value=metric_value, step=self.curr_epoch)
+        else:
+            raise ValueError(f'Split invalid: {split}')
+
     @property
     def eval_state(self) -> TrainState:
         if self.config.train.schedule_free:
@@ -253,6 +292,13 @@ class TrainingRun:
                 self.state = self.state.replace(
                     params=best_ckpt(self.config.restart_from)['state']['params']
                 )
+
+            # log number of parameters
+            self.run['params'] = int(
+                jax.tree.reduce(
+                    lambda x, y: x + y, jax.tree.map(lambda x: x.size, self.state.params)
+                )
+            )
         elif step >= self.num_steps:
             return None
 
@@ -267,42 +313,44 @@ class TrainingRun:
         # jax.debug.visualize_array_sharding(preds[..., 0])
         self.state = self.compute_metrics(preds=preds, state=self.eval_state, **kwargs)
 
-        if (
-            self.should_log or self.should_ckpt or self.should_validate
-        ):  # one training epoch has passed
+        if self.should_log or self.should_ckpt or self.should_validate:
             for metric, value in self.state.metrics.items():  # compute metrics
                 if metric == 'grad_norm':
-                    self.metrics_history['grad_norm'].append(value)  # record metrics
+                    self.log_metric('grad_norm', value, None)
                     continue
-                self.metrics_history[f'tr_{metric}'].append(value)  # record metrics
+                self.log_metric(metric, value, 'train')
 
                 if not self.should_validate:
                     if f'te_{metric}' in self.metrics_history:
-                        self.metrics_history[f'te_{metric}'].append(
-                            self.metrics_history[f'te_{metric}'][-1]
-                        )
+                        test_value = self.metrics_history[f'te_{metric}'][-1]
                     else:
-                        self.metrics_history[f'te_{metric}'].append(0)
+                        test_value = 0
+                    self.log_metric(metric, test_value, 'valid')
 
             self.state = self.state.replace(
                 metrics=Metrics()
             )  # reset train_metrics for next training epoch
 
-            self.metrics_history['lr'].append(self.lr)
-            self.metrics_history['step'].append(self.curr_step)
-            self.metrics_history['epoch'].append(self.curr_step / self.steps_in_epoch)
-            self.metrics_history['rel_mins'].append((time.monotonic() - self.start_time) / 60)
+            self.log_metric('lr', self.lr, None)
+            self.log_metric('step', self.curr_step, None)
+            self.log_metric('epoch', self.curr_epoch, None)
+            self.log_metric('rel_mins', (time.monotonic() - self.start_time) / 60, None)
             if max(self.metrics_history['epoch'], default=0) < 1:
-                self.metrics_history['throughput'].append(
-                    self.curr_step * self.config.batch_size / self.metrics_history['rel_mins'][-1]
+                self.log_metric(
+                    'throughput',
+                    self.curr_step
+                    * self.config.batch_size
+                    * self.config.stack_size
+                    / self.metrics_history['rel_mins'][-1],
+                    None,
                 )
             else:
                 prev, curr = self.metrics_history['rel_mins'][-2:]
                 min_delta = curr - prev
 
                 prev, curr = self.metrics_history['step'][-2:]
-                size = (curr - prev) * self.config.batch_size
-                self.metrics_history['throughput'].append(size / min_delta)
+                size = (curr - prev) * self.config.batch_size * self.config.stack_size
+                self.log_metric('throughput', size / min_delta, None)
 
         # debug_structure(self.state)
         # print(self.metrics_history)
@@ -344,7 +392,7 @@ class TrainingRun:
             for metric, value in self.test_state.metrics.items():
                 if metric == 'grad_norm':
                     continue
-                self.metrics_history[f'te_{metric}'].append(value)
+                self.log_metric(metric, value, 'valid')
                 if f'{metric}' == 'loss':
                     self.test_loss = value
 
@@ -366,13 +414,20 @@ class TrainingRun:
         for step, batch in zip(self.steps, self.dl):
             self.step(step, batch)
 
+    @property
+    def curr_epoch(self) -> float:
+        return self.curr_step / self.steps_in_epoch
+
     @property
     def should_log(self):
         return (self.curr_step + 1) % (self.steps_in_epoch // self.config.log.logs_per_epoch) == 0
 
     @property
     def should_ckpt(self):
-        return (self.curr_step + 1) % (self.config.log.epochs_per_ckpt * self.steps_in_epoch) == 0
+        right_step = (self.curr_step + 1) % (
+            self.config.log.epochs_per_ckpt * self.steps_in_epoch
+        ) == 0
+        return right_step and not self.config.debug_mode
 
     @property
     def should_validate(self):
@@ -390,8 +445,9 @@ class TrainingRun:
 
     def save_final(self, out_dir: str | PathLike):
         """Save final model to directory."""
-        self.mngr.wait_until_finished()
-        copytree(self.mngr.directory, Path(out_dir) / 'ckpts/')
+        if not self.config.debug_mode:
+            self.mngr.wait_until_finished()
+            copytree(self.mngr.directory, Path(out_dir) / 'ckpts/')
 
     def finish(self):
         now = datetime.now()
diff --git a/cdv/utils.py b/cdv/utils.py
index 1933da6..8808411 100644
--- a/cdv/utils.py
+++ b/cdv/utils.py
@@ -371,9 +371,9 @@ def flax_summary(
         color = INFERNA[color_i]
         t = Text(s.strip(), style=Style(color=color, bold=(e > 3)))
         f = io.StringIO()
-        console = rich.console.Console(
-            file=f, color_system=rich.console.Console().color_system, **(console_kwargs or {})
-        )
+        kwargs = {'width': rich.get_console().width}
+        kwargs.update(console_kwargs or {})
+        console = rich.console.Console(file=f, color_system='truecolor', **kwargs)
         console.print(t, end='')
         return f.getvalue()
 
diff --git a/configs/defaults.toml b/configs/defaults.toml
index 84291f1..f1f6243 100644
--- a/configs/defaults.toml
+++ b/configs/defaults.toml
@@ -2,6 +2,7 @@ batch_size = 32
 stack_size = 1
 do_profile = false
 num_epochs = 20
+debug_mode = false
 display = "dashboard"
 regressor = "mace"
 task = "e_form"
diff --git a/configs/test.toml b/configs/test.toml
index a07bba6..e7de8f7 100644
--- a/configs/test.toml
+++ b/configs/test.toml
@@ -1,6 +1,7 @@
-batch_size = 32
+batch_size = 64
 stack_size = 1
 do_profile = false
+debug_mode = true
 num_epochs = 5
 display = "dashboard"
 regressor = "mace"
@@ -57,9 +58,9 @@ num_basis = 8
 r_max = 6
 avg_num_neighbors = 14
 max_ell = 2
-hidden_irreps = ["128x0e + 64x1o + 32x2e", "128x0e + 64x1o + 32x2e"]
+hidden_irreps = ["128x0e + 64x1o + 32x2e", "256x0e"]
 interaction_reduction = "last"
-node_out_dim = 64
+outs_per_node = 64
 share_species_embed = true
 
 [mace.head]
diff --git a/env.sh b/env.sh
index c96f5d0..273c26b 100644
--- a/env.sh
+++ b/env.sh
@@ -22,4 +22,4 @@ export XLA_FLAGS="
 export JAX_TRACEBACK_IN_LOCATIONS_LIMIT=20
 export XLA_PYTHON_CLIENT_MEM_FRACTION=0.9
 source secrets.sh
-export NEPTUNE_PROJECT="facet"
\ No newline at end of file
+export NEPTUNE_PROJECT="facet/energy"
\ No newline at end of file
